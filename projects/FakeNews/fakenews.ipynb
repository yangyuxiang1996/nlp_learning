{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596160103651",
   "display_name": "Python 3.8.3 64-bit ('torchserve': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['./data/train.csv.zip', './data/test.csv.zip']"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob('./data/*.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id  tid1  tid2                          title1_zh  \\\n0   0     0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n1   3     2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n2   1     2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n3   2     2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n4   9     6     7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n\n                    title2_zh  \\\n0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n2        GDP首超香港？深圳澄清：还差一点点……   \n3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n\n                                           title1_en  \\\n0  There are two new old-age insurance benefits f...   \n1  \"If you do not come to Shenzhen, sooner or lat...   \n2  \"If you do not come to Shenzhen, sooner or lat...   \n3  \"If you do not come to Shenzhen, sooner or lat...   \n4  \"How to discriminate oil from gutter oil by me...   \n\n                                           title2_en      label  \n0  Police disprove \"bird's nest congress each per...  unrelated  \n1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n2  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n3  Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n4  It took 30 years of cooking oil to know that o...     agreed  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tid1</th>\n      <th>tid2</th>\n      <th>title1_zh</th>\n      <th>title2_zh</th>\n      <th>title1_en</th>\n      <th>title2_en</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n      <td>There are two new old-age insurance benefits f...</td>\n      <td>Police disprove \"bird's nest congress each per...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n      <td>\"How to discriminate oil from gutter oil by me...</td>\n      <td>It took 30 years of cooking oil to know that o...</td>\n      <td>agreed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "os.system('unzip ./data/train.csv.zip')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0         False\n1         False\n2         False\n3         False\n4         False\n          ...  \n320547    False\n320548    False\n320549    False\n320550    False\n320551    False\nLength: 320552, dtype: bool"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "empty_title = ((df_train['title1_zh'].isnull()) \\\n",
    "         | (df_train['title2_zh'].isnull()) \\\n",
    "         | (df_train['title2_zh'] == '') \\\n",
    "         | (df_train['title2_zh'] == '0'))\n",
    "empty_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           text_a                          text_b      label\n0              8张图片能证明张柏芝不会和谢霆锋复婚        看看张柏芝散发出的母爱 就知道所有新恋情都是谣言  unrelated\n1  普洱茶致癌、中式咸鱼致癌？院士专家解读2017年食品安全热点  邢台金沙河挂面掺胶？谣言！这些有关食品的说法都是谣言不要信！  unrelated\n2          联想的5G投票最后为何没给华为，是什么意思？  联想投票反对预装国产系统 联想辟谣：我们反对的只是双系统方案  unrelated\n3  娱乐猛料：唐嫣罗晋被曝分手，网友说又被玩了！老铁们你们怎么看   玩雪伤眼？兰州拉面致癌？一月“十大谣言”出炉，你上当了吗？  unrelated\n4        抗流感病毒药奥司他韦多地断货！国产药正在加速上市                    流感严重，奥司他韦卖断货     agreed",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_a</th>\n      <th>text_b</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8张图片能证明张柏芝不会和谢霆锋复婚</td>\n      <td>看看张柏芝散发出的母爱 就知道所有新恋情都是谣言</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>普洱茶致癌、中式咸鱼致癌？院士专家解读2017年食品安全热点</td>\n      <td>邢台金沙河挂面掺胶？谣言！这些有关食品的说法都是谣言不要信！</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>联想的5G投票最后为何没给华为，是什么意思？</td>\n      <td>联想投票反对预装国产系统 联想辟谣：我们反对的只是双系统方案</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>娱乐猛料：唐嫣罗晋被曝分手，网友说又被玩了！老铁们你们怎么看</td>\n      <td>玩雪伤眼？兰州拉面致癌？一月“十大谣言”出炉，你上当了吗？</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>抗流感病毒药奥司他韦多地断货！国产药正在加速上市</td>\n      <td>流感严重，奥司他韦卖断货</td>\n      <td>agreed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train = df_train[~empty_title]\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "df_train = df_train[~(df_train.title1_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "df_train = df_train[~(df_train.title2_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "\n",
    "SAMPLE_RATE = 0.01\n",
    "df_train = df_train.sample(frac=SAMPLE_RATE, random_state=2020)\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train samples:  2657\n"
    }
   ],
   "source": [
    "df_train.to_csv('train.tsv', sep='\\t', index=False)\n",
    "print('train samples: ', len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "unrelated    0.674068\nagreed       0.297704\ndisagreed    0.028227\nName: label, dtype: float64"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_train.label.value_counts() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test samples:  80126\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                            text_a                       text_b      Id\n0  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？  321187\n1              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国  321190\n2    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思  321189\n3              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！  321193\n4              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！  321191",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_a</th>\n      <th>text_b</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n      <td>321187</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n      <td>321190</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>321189</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n      <td>321193</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n      <td>321191</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "os.system('unzip ./data/test.csv.zip')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.loc[:, ['title1_zh', 'title2_zh', 'id']]\n",
    "df_test.columns = ['text_a', 'text_b', 'Id']\n",
    "df_test.to_csv('test.tsv', sep='\\t', index=False)\n",
    "\n",
    "print('test samples: ', len(df_test))\n",
    "df_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "測試集樣本數 / 訓練集樣本數 = 30.2 倍\n"
    }
   ],
   "source": [
    "ratio = len(df_test) / len(df_train)\n",
    "print(\"測試集樣本數 / 訓練集樣本數 = {:.1f} 倍\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "import torch\n",
    "import pysnooper\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train', 'dev', 'test']\n",
    "        self.mode = mode\n",
    "\n",
    "        self.df = pd.read_csv('%s.tsv' % mode, sep='\\t').fillna('')\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # @pysnooper.snoop()\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'test':\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        \n",
    "        word_pieces = ['[CLS]']\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + ['[SEP]']\n",
    "        len_a = len(tokens_a)\n",
    "\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + ['[SEP]']\n",
    "        len_b = len(word_pieces) - len_a\n",
    "\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, dtype=torch.long)\n",
    "\n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "trainset = FakeNewsDataset('train', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[原始文本]\n句子 1：8张图片能证明张柏芝不会和谢霆锋复婚\n句子 2：看看张柏芝散发出的母爱 就知道所有新恋情都是谣言\n分類  ：unrelated\n\n--------------------\n\n[Dataset 回傳的 tensors]\ntokens_tensor  ：tensor([ 101,  129, 2476, 1745, 4275, 5543, 6395, 3209, 2476, 3377, 5698,  679,\n         833, 1469, 6468, 7447, 7226, 1908, 2042,  102, 4692, 4692, 2476, 3377,\n        5698, 3141, 1355, 1139, 4638, 3678, 4263, 2218, 4761, 6887, 2792, 3300,\n        3173, 2605, 2658, 6963, 3221, 6469, 6241,  102])\n\nsegments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\nlabel_tensor   ：2\n\n--------------------\n\n[還原 tokens_tensors]\n[CLS]8张图片能证明张柏芝不会和谢霆锋复婚[SEP]看看张柏芝散发出的母爱就知道所有新恋情都是谣言[SEP]\n\n"
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx, :].values\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = ''.join(tokens)\n",
    "\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text_a}\n",
    "句子 2：{text_b}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples]) # Concatenates sequence of tensors along a new dimension.\n",
    "    else:\n",
    "        label_ids = None\n",
    "\n",
    "    # Pad a list of variable length Tensors with padding_value\n",
    "    # This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. \n",
    "    # This function assumes trailing dimensions and type of all the Tensors in sequences are same.\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True) \n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "\n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\ntokens_tensors.shape   = torch.Size([64, 62]) \ntensor([[ 101,  129, 2476,  ...,    0,    0,    0],\n        [ 101, 3249, 3827,  ...,  102,    0,    0],\n        [ 101, 5468, 2682,  ...,    0,    0,    0],\n        ...,\n        [ 101, 3557, 3207,  ...,    0,    0,    0],\n        [ 101, 8267, 2259,  ...,    0,    0,    0],\n        [ 101, 1600, 5763,  ...,    0,    0,    0]])\n------------------------\nsegments_tensors.shape = torch.Size([64, 62])\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 1, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n------------------------\nmasks_tensors.shape    = torch.Size([64, 62])\ntensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\n------------------------\nlabel_ids.shape        = torch.Size([64])\ntensor([2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n        2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0,\n        0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2])\n\n"
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "name      module         \nbert:embeddings\nbert:encoder\nbert:pooler\ndropout        Dropout(p=0.1, inplace=False)\nclassifier     Linear(in_features=768, out_features=3, bias=True)\n"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_PATH = './bert-base-chinese/'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_PATH, num_labels = NUM_LABELS)\n",
    "clear_output()\n",
    "\n",
    "print('{0:10}{1:15}'.format('name', 'module'))\n",
    "for name, module in model.named_children():\n",
    "    if name == 'bert':\n",
    "        for n, _ in module.named_children():\n",
    "            print(f'{name}:{n}')\n",
    "    else:\n",
    "        print('{:15}{}'.format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if next(model.parameters()).is_cuda: # check if the model is on cuda\n",
    "                data = [t.to(torch.device('cuda:0')) for t in data if t is not None]\n",
    "\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "\n",
    "        if compute_acc:\n",
    "            acc = correct / total\n",
    "            return predictions, acc\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "device:  cuda:0\nclassification acc:  0.48550997365449755\n"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print('classification acc: ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n整個分類模型的參數量：102269955\n線性分類器的參數量：2307\n\n"
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "\n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[epoch 1] loss: 24.194, acc: 0.826\n[epoch 2] loss: 18.037, acc: 0.881\n[epoch 3] loss: 14.730, acc: 0.912\n[epoch 4] loss: 10.960, acc: 0.937\n[epoch 5] loss: 8.292, acc: 0.951\n[epoch 6] loss: 7.400, acc: 0.967\nCPU times: user 1min 13s, sys: 23.7 s, total: 1min 37s\nWall time: 1min 37s\n"
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 6\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors,\n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss\n",
    "\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %(epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Id   Category\n0  321187  unrelated\n1  321190  unrelated\n2  321189  unrelated\n3  321193  unrelated\n4  321191  unrelated",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>321187</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>321190</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>321189</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>321193</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>321191</td>\n      <td>unrelated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "testset = FakeNewsDataset('test', tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n",
    "\n",
    "predictions = get_predictions(model, testloader)\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "df = pd.DataFrame({\"Category\" : predictions.tolist()})\n",
    "df['Category'] = df.Category.apply(lambda x: index_map[x])\n",
    "df_pred = pd.concat([testset.df.loc[:, 'Id'], df.loc[:, 'Category']], axis = 1)\n",
    "df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         text_a                         text_b      label  predicted\n1174   张卫健吸毒被带走             网传张卫健吸毒被捕？发微博力破谣言！  disagreed  disagreed\n1365   腾讯即将开始封群      辟谣｜腾讯即将开始封群 最近千万别在群里发任何东西  disagreed  disagreed\n1514  第六套人民币亮相了       「辟谣」第六套新版人民币亮相了？别再被假象蒙蔽！  disagreed  disagreed\n2369   张卫健吸毒被带走  网爆张卫健吸毒被抓是谣言，爆料者已道歉，并已移交警方做笔录  disagreed  disagreed",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_a</th>\n      <th>text_b</th>\n      <th>label</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1174</th>\n      <td>张卫健吸毒被带走</td>\n      <td>网传张卫健吸毒被捕？发微博力破谣言！</td>\n      <td>disagreed</td>\n      <td>disagreed</td>\n    </tr>\n    <tr>\n      <th>1365</th>\n      <td>腾讯即将开始封群</td>\n      <td>辟谣｜腾讯即将开始封群 最近千万别在群里发任何东西</td>\n      <td>disagreed</td>\n      <td>disagreed</td>\n    </tr>\n    <tr>\n      <th>1514</th>\n      <td>第六套人民币亮相了</td>\n      <td>「辟谣」第六套新版人民币亮相了？别再被假象蒙蔽！</td>\n      <td>disagreed</td>\n      <td>disagreed</td>\n    </tr>\n    <tr>\n      <th>2369</th>\n      <td>张卫健吸毒被带走</td>\n      <td>网爆张卫健吸毒被抓是谣言，爆料者已道歉，并已移交警方做笔录</td>\n      <td>disagreed</td>\n      <td>disagreed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "predictions = get_predictions(model, trainloader)\n",
    "df = pd.DataFrame({'predicted': predictions.tolist()})\n",
    "df['predicted'] = df.predicted.apply(lambda x: index_map[x])\n",
    "df1 = pd.concat([trainset.df, df.loc[:, 'predicted']], axis=1)\n",
    "disagreed_tp = ((df1.label == 'disagreed') & \\\n",
    "     (df1.label == df1.predicted) & \\\n",
    "     (df1.text_a.apply(lambda x: True if len(x) < 10 else False)))\n",
    "df1[disagreed_tp].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝 BertViz\n",
    "import sys\n",
    "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
    "if not 'bertviz_repo' in sys.path:\n",
    "  sys.path += ['bertviz_repo']\n",
    "\n",
    "# import packages\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "# 在 jupyter notebook 裡頭顯示 visualzation 的 helper\n",
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model_version = 'bert-base-chinese'\n",
    "finetuned_model = BertModel.from_pretrained(model_version, output_attention=True, state_dict=model.state_dict())\n",
    "\n",
    "# 兩個句子\n",
    "sentence_a = \"烟王褚时健去世\"\n",
    "sentence_b = \"辟谣：一代烟王褚时健安好！\"\n",
    "\n",
    "# 得到 tokens 後丟入 BERT 取得 attention\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "input_ids = inputs['input_ids']\n",
    "attention = finetuned_model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "input_id_list = input_ids[0].tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "call_html()\n",
    "head_view(attention, tokens)\n",
    "\n",
    "\n"
   ]
  }
 ]
}